<!DOCTYPE html>
<!-- saved from url=(0052)https://raaslab.org/projects/LLM_Context_Estimation/ -->
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="description" content="Leveraging Language for Context-Aware">
  <meta name="keywords" content="AI-based Learning, Reinforcement Learning, LLM, Context-Awareness">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LANCAR: Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments</title>

  <script src="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/plotly-latest.min.js.download"></script><style type="text/css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;-webkit-box-sizing:border-box;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.3333333333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:solid .08em #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s infinite linear;animation:fa-spin 2s infinite linear}.fa-pulse{-webkit-animation:fa-spin 1s infinite steps(8);animation:fa-spin 1s infinite steps(8)}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{-webkit-transform:scale(1,-1);transform:scale(1,-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1,-1);transform:scale(-1,-1)}:root .fa-flip-both,:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-rotate-90{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2.5em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1.25em}.svg-inline--fa.fa-stack-2x{height:2em;width:2.5em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}.svg-inline--fa .fa-primary{fill:var(--fa-primary-color,currentColor);opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa .fa-secondary{fill:var(--fa-secondary-color,currentColor);opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-primary{opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-secondary{opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa mask .fa-primary,.svg-inline--fa mask .fa-secondary{fill:#000}.fad.fa-inverse{color:#fff}</style><style id="plotly.js-style-global"></style>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/css" rel="stylesheet">

  <link rel="stylesheet" href="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/bulma.min.css">
  <link rel="stylesheet" href="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/bulma-carousel.min.css">
  <link rel="stylesheet" href="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/bulma-slider.min.css">
  <link rel="stylesheet" href="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/fontawesome.all.min.css">
  <link rel="stylesheet" href="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/academicons.min.css">
  <link rel="stylesheet" href="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/index.css">
  <link rel="icon" href="https://raaslab.org/projects/LLM_Context_Estimation/static/images/favicon.svg">

  <script src="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/jquery.min.js.download"></script>
  <script defer="" src="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/fontawesome.all.min.js.download"></script>
  <script src="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/bulma-carousel.min.js.download"></script>
  <script src="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/bulma-slider.min.js.download"></script>
  <script src="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/index.js.download"></script>
  <script src="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/htmlincludejs"></script>

</head>
<body data-new-gr-c-s-check-loaded="14.1155.0" data-gr-ext-installed="">

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="http://raaslab.org/">
      <span class="icon">
          <svg class="svg-inline--fa fa-home fa-w-18" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="home" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512" data-fa-i2svg=""><path fill="currentColor" d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg><!-- <i class="fas fa-home"></i> Font Awesome fontawesome.com -->
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Table of Contents
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://raaslab.org/projects/LLM_Context_Estimation/#abstract">
            Abstract
          </a>
          <a class="navbar-item" href="https://raaslab.org/projects/LLM_Context_Estimation/#video">
            Video
          </a>
          <a class="navbar-item" href="https://raaslab.org/projects/LLM_Context_Estimation/#method">
            Method
          </a>
          <a class="navbar-item" href="https://raaslab.org/projects/LLM_Context_Estimation/#simulation">
            Simulation
          </a>
          <a class="navbar-item" href="https://raaslab.org/projects/LLM_Context_Estimation/#results">
            Results
          </a>
          <a class="navbar-item" href="https://raaslab.org/projects/LLM_Context_Estimation/#relatedWorks">
            Related Works
          </a>
        </div>
      </div>

    </div>

  </div>
</nav>



<!-- https://github.com/raaslab/Pred-NBV -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LANCAR: Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://raaslab.org/projects/LLM_Context_Estimation/">Chak Lam Shek</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://raaslab.org/projects/LLM_Context_Estimation/">Xiyang Wu</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://raaslab.org/projects/LLM_Context_Estimation/">Dinesh Manocha</a>,</span>
            <span class="author-block">
              <a href="http://tokekar.com/">Pratap Tokekar</a>,</span>
            <span class="author-block">
              <a href="https://raaslab.org/projects/LLM_Context_Estimation/">Amrit Singh Bedi</a>
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Maryland, College Park</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution (listed alphabatically)</span>
          </div>
        
          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Submission</span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <!-- Arxiv Link -->
              <span class="link-block">
                <a href="https://raaslab.org/projects/LLM_Context_Estimation/" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <!--
              <span class="link-block">
                <a href="https://github.com/raaslab/Pred-NBV"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://obj.umiacs.umd.edu/publicdata/PoinTr_C_final.pth"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Model</span>
                  </a>
              </span>
            </div>
          </div> -->


          </div>
        </div>
      </div>
    </div>
  </div>
</div></section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/task.png" class="interpolation-image" alt="Interpolation end reference image.">


    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><a id="abstract">Abstract</a></h2>
        <div class="content has-text-justified ">
          <p>
          Robotic locomotion is a challenging task, especially in unstructured terrains.
          In practice, the optimal locomotion policy can be context-dependent by using the contextual information of encountered terrains in decision-making. Humans can interpret the environmental context for robots, but the ambiguity of human language makes it challenging to use in robot locomotion directly. In this paper, we propose a novel approach that introduces a context translator that works with reinforcement learning (RL) agents for context-aware locomotion. Our formulation allows a robot to interpret the contextual information from environments generated by human observers or Vision-Language Models (VLM) with Large Language Models (LLM) and use this information to generate contextual embeddings. We incorporate the contextual embeddings with the robot's internal environmental observations as the input to the RL agent's decision neural network.
          We evaluate with contextual information in varying ambiguity levels and compare its performance using several alternative approaches.
          Our experimental results demonstrate that our approach exhibits good generalizability and adaptability across diverse terrains, by achieving at least <b>10%</b> of performance improvement in episodic reward over baselines.
          </p>
        </div>
      </div>
    </div>
  <br>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><a id="video">Video</a></h2>
        <div class="publication-video">
          <video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%">
        <source src="./videos/ICRA 2024.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="hero is-light">
  <div class="hero-body">
    <div class="container has-text-justified">
     <h2 class="title is-3 has-text-centered"><br><a id="method">Method</a></h2>
     <div>
          <img src="./LANCAR_ Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments_files/frame.png" alt="Interpolation end reference image." style="padding: 60px;width:100%;height:100%;margin: auto;padding-left: 80px;">


     </div>
     
     <p> Context-Aware Reinforcement Learning Robot Locomotion. Our framework introduces a context translator aside from the
     standard RL framework. For the environment with diverse terrains,
     the agent gets the explicitly observable state as the observation, and
     the human observer (or VLM) perceives the context information
     as the implicitly observable state. The human observer (or VLM)
     interprets the contextual information to the LLM translator. The
     LLM translator extracts the environmental properties from the con-
     textual information and generates the contextual embedding, which
     is concatenated with the observations as the input for RL agents.
     RL agents produce the action using their control policies given the
     context-aware inputs and execute the action in the environment. </p>
       <br>
      
  </div>
</div></section>


<section class="hero  is-small">
  <div class="container is-max-desktop">
    <!-- Qual Results -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"><br><a id="simulation">Simulation</a></h2>
         <p> </p>
       <br>
       <div class="column is-full-width">
           <h3> Case 1: Low Friction </h3>
       </div>
          <div class="row">
            <div class="column">
              <p align="center"><b>Baseline</b></p>
              <div class="publication-video">
                <video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%">
              <source src="./videos/Case1_Basic.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <div class="column">
              <p align="center"><b>Indexing</b></p>
              <div class="publication-video">
                <video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%">
              <source src="./videos/Case1_indexing.mp4" type="video/mp4">
                </video>
              </div>

            </div>
            <div class="column">
              <p align="center"><b>LANCAR</b></p>
              <div class="publication-video">
                <video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%">
              <source src="./videos/Case1_LANCAR.mp4" type="video/mp4">
                </video>
              </div>

            </div>
          </div>
        <br>
        <div class="column is-full-width">
            <h3> Case 2: Elastic Ground </h3>
        </div>
           <div class="row">
             <div class="column">
               <p align="center"><b>Baseline</b></p>
               <div class="publication-video">
                 <video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%">
               <source src="./videos/Case2_Basic.mp4" type="video/mp4">
                 </video>
               </div>
             </div>
             <div class="column">
               <p align="center"><b>Indexing</b></p>
               <div class="publication-video">
                 <video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%">
               <source src="./videos/Case2_indexing.mp4" type="video/mp4">
                 </video>
               </div>

             </div>
             <div class="column">
               <p align="center"><b>LANCAR</b></p>
               <div class="publication-video">
                 <video id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%">
               <source src="./videos/Case2_LANCAR.mp4" type="video/mp4">
                 </video>
               </div>

             </div>
           </div>
         <br>
    </div>
  </div>
</div>
</section>

<section class="hero is-light">
  <div class="hero-body">
    <div class="container has-text-justified">
      <br>
     <h2 class="title is-3 has-text-centered"><a id="results">Results</a></h2>
     <p>Comparison of LANCAR against the baseline.</p>
      <table class="is-max-desktop" align="center" cellpadding="10" cellspacing="10">
        <thead class="is-8">
          <tr style="text-align: right">
            <th></th>
            <th></th>
            <!-- <th></th> -->
            <th>Total Rewards in 10^3 &nbsp;</th>
            <th>&nbsp;(5000 steps)</th>
            <th></th>
          </tr>
          <tr style="border-bottom: 1px solid black">
            <th> Case </th>
            <!-- <th>Demo</th> -->
            <th>Context ID</th>
            <th>No-Contex Method</th>
            <th>Indexing Method</th>
            <th>LANCAR</th>
          </tr>
        </thead>
        <tbody class="is-8" style="border-bottom: 1px solid black">
          <tr>
            <th></th>
            <td>A</td>
            <!-- <td><img width="25%" src="./static/images/models/747.png"></td> -->
            <td>25.661 ± 0.475</td>
            <td>28.083 ± 1.056</td>
            <td>31.886 ± 0.433</td>

          </tr>
          <tr>
            <th>Low-Level</th>
            <td>B</td>
            <!-- <td><img width="25%" src="./static/images/models/A340.png"></td> -->
            <td>10.612 ± 1.720</td>
            <td>10.551 ± 1.700</td>
            <td>34.919 ± 0.340</td>

          </tr>
          <tr>
            <th></th>
            <td>C</td>
            <!-- <td><img width="25%" src="./static/images/models/C17.png"></td> -->
            <td>29.853 ± 0.443</td>
            <td>34.210 ± 0.411</td>
            <td>34.915 ± 0.352</td>
          </tr>
          <tr style="border-top: 1px solid black">
            <th></th>
            <td>D</td>
            <!-- <td><img width="5%" src="./static/images/models/Atlas.png"></td> -->
            <td>29.592 ± 0.367 </td>
            <td>33.737 ± 0.563</td>
            <td>33.725 ± 0.414</td>
          </tr>
          <tr>
            <th> High-Level</th>
            <td>E</td>
            <!-- <td><img width="5%" src="./static/images/models/Maverick.png"></td> -->
            <td>25.726 ± 1.732</td>
            <td>8.718 ± 1.988</td>
            <td>30.332 ± 2.130</td>
          </tr>
          <tr>
            <th></th>
            <td>F</td>
            <!-- <td><img width="5%" src="./static/images/models/SaturnV.png"></td> -->
            <td>29.821 ± 0.434</td>
            <td>34.294 ± 0.397</td>
            <td>34.917 ± 0.272</td>
          </tr>
          <tr style="border-top: 1px solid black">
            <th>VLM</th>
            <td>G</td>
            <!-- <td><img width="5%" src="./static/images/models/BigBen.png"></td> -->
            <td>29.913 ± 0.568</td>
            <td>31.942 ± 0.828</td>
            <td>33.112 ± 0.583</td>
          </tr>
          <tr>
            <th></th>
            <td>H</td>
            <!-- <td><img width="5%" src="./static/images/models/ChurchTower.png"></td> -->
            <td>29.788 ± 0.391</td>
            <td>34.142 ± 0.368</td>
            <td>34.142 ± 0.441</td>
          </tr>
        </tbody>
      </table>

      
      
    </div>
  </div>
</section>

<!-- <section class="hero" is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"></br>Qualitative Results</h2>
         <p> We compare <a src="https://arxiv.org/abs/2108.08839">PoinTr</a>, a 3D shape completion, against PoinTr-C, a improved version of PoinTr which we train with curriculum learning to make robust against perturbations in the canonical point cloud representations. The results below show shape completion results on data from ShapeNet dataset, when these pertubations in injected in the point clouds.</p>
       </br>
          <div class="row">
            <div class="column">
              <p align='center'><b>Input</b></p>
              <img width="97%" src="./static/images/models/747.png">
              <img width="50%" src="./static/images/models/Atlas.png">
              <img width="97%" src="./static/images/models/BigBen.png">
              <img width="97%" src="./static/images/models/Diesel.png">
            </div>
            <div class="column">
              <p align='center'><b>Ground Truth</b></p>
              <img width="97%" src="./static/images/models/A340.png">
              <img width="50%" src="./static/images/models/Maverick.png">
              <img width="97%" src="./static/images/models/Church.png">
              <img width="97%" src="./static/images/models/Mountain.png">
            </div>
            <div class="column">
              <p align='center'><b>PoinTr</b></p>
              <img width="97%" src="./static/images/models/C17.png">
              <img width="50%" src="./static/images/models/SaturnV.png">
              <img width="97%" src="./static/images/models/Clock.png">
              <img width="97%" src="./static/images/models/Cruise.png">
              <img width="97%" src="./static/images/models/Silo.png">
            </div>
            <div class="column">
              <p align='center'><b>PoinTr-C (Ours)</b></p>
              <img width="97%" src="./static/images/models/C130.png">
              <img width="50%" src="./static/images/models/Sparrow.png">
              <img width="97%" src="./static/images/models/Pylon.png">
              <img width="97%" src="./static/images/models/Patrol.png">
            </div>
            <div class="column">
              <p align='center'><b>PoinTr-C (Ours)</b></p>
              <img width="97%" src="./static/images/models/Fokker100.png">
              <img width="50%" src="./static/images/models/v2.png">
              <img width="97%" src="./static/images/models/Pylon.png">
              <img width="97%" src="./static/images/models/Yacht.png">
            </div>
          </div>
        </br>
    </div>
  </div>
</div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"><a id="relatedWorks">Related Works</a></h2>

        <div class="content has-text-justified">
          <p>
            <a href="https://arxiv.org/abs/2109.01134">Learning to Prompt for Vision-Language Models</a>. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu. IJCV 2022.
          </p>
          <p>
            <a href="https://arxiv.org/abs/1711.07280">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</a>. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton van den Hengel. CVPR 2018.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2307.15818">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control
                </a>. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich. 2023
          </p>
        </div>
      </div>
    </div>

  </div>
</section>





<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          <p></p>
        </div>
      </div>
    
  
</footer>



</body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>